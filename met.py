import pandas as pd
import joblib
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score

print("--- ğŸ“Š GÃ‰NÃ‰RATEUR DE MÃ‰TRIQUES POUR LA SOUTENANCE ---")

# --- 1. CONFIGURATION (Indispensable pour charger le modÃ¨le) ---
# On doit redÃ©finir les fonctions utilisÃ©es dans le pickle si elles ne sont pas importÃ©es
try:
    from journal_utils import get_text_data, get_numeric_data
except ImportError:
    # Au cas oÃ¹ le fichier n'est pas lÃ , on le recrÃ©e Ã  la volÃ©e
    utils_code = """
import pandas as pd
def get_text_data(x):
    return x['Titre'].astype(str) + " " + x['Publisher'].astype(str)
def get_numeric_data(x):
    return x[['oa_works', 'oa_cited', 'oa_found', 'cr_has_doi', 'Impact_Ratio']]
"""
    with open("journal_utils.py", "w") as f:
        f.write(utils_code)
    from journal_utils import get_text_data, get_numeric_data

# --- 2. CHARGEMENT DES DONNÃ‰ES & DU MODÃˆLE ---
print("1ï¸âƒ£ Chargement du modÃ¨le et des donnÃ©es...")
model = joblib.load('model_xgboost_publisher.pkl')
df = pd.read_csv("data/02_real_world_dataset.csv")

# Nettoyage (Identique Ã  l'entraÃ®nement)
df['Publisher'] = df['Publisher'].fillna('Unknown')
df['Titre'] = df['Titre'].fillna('')
cols_num = ['oa_works', 'oa_cited', 'oa_found', 'cr_has_doi', 'Impact_Ratio']
for col in cols_num:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# --- 3. RECRÃ‰ATION DU TEST SET ---
# On utilise le mÃªme random_state=42 pour retrouver exactement les mÃªmes donnÃ©es de test
X = df
y = df['Est_Predateur']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# --- 4. CALCUL DU SEUIL OPTIMAL ---
# On recalcule le seuil exact qui a Ã©tÃ© trouvÃ© lors de l'entraÃ®nement
probs = model.predict_proba(X_test)[:, 1]
best_threshold = 0.5
best_f1 = 0
for threshold in np.arange(0.3, 0.7, 0.01):
    preds = (probs >= threshold).astype(int)
    f1 = f1_score(y_test, preds)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = threshold

print(f"   â¤ Seuil Optimal RecalculÃ© : {best_threshold:.2f}")
y_pred = (probs >= best_threshold).astype(int)

# --- 5. AFFICHAGE DES RÃ‰SULTATS (Format Slide) ---
acc = accuracy_score(y_test, y_pred)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
report = classification_report(y_test, y_pred, output_dict=True)

print("\n" + "="*40)
print("   ğŸ“ CHIFFRES CLÃ‰S POUR LA SLIDE 4")
print("="*40)

print(f"\nğŸ”¹ 1. PERFORMANCE GLOBALE (Accuracy)")
print(f"   Score : {acc:.2%} (C'est ce chiffre qu'il faut mettre en GROS)")

print(f"\nğŸ”¹ 2. MATRICE DE CONFUSION (La Preuve)")
print(f"   âœ… Vrais Positifs (Arnaques stoppÃ©es) : {tp}")
print(f"   ğŸ›¡ï¸ Vrais NÃ©gatifs (Revues fiables OK) : {tn}")
print(f"   âš ï¸ Faux Positifs (Fausses alertes)    : {fp}")
print(f"   â˜ ï¸ Faux NÃ©gatifs (Arnaques ratÃ©es)    : {fn}  <-- LE PLUS IMPORTANT (Doit Ãªtre bas)")

print(f"\nğŸ”¹ 3. DÃ‰TAILS PAR CLASSE")
print(f"   ğŸ”´ Classe 'PrÃ©dateur' :")
print(f"      - PrÃ©cision : {report['1']['precision']:.2f}")
print(f"      - Rappel    : {report['1']['recall']:.2f} (CapacitÃ© Ã  tout dÃ©tecter)")
print(f"      - F1-Score  : {report['1']['f1-score']:.2f}")

print("\n" + "="*40)
print("ğŸ’¡ PHRASE D'ANALYSE POUR LE PROF :")
print(f"> 'Avec un seuil de {best_threshold:.2f}, nous avons rÃ©ussi Ã  bloquer {tp} revues prÃ©datrices")
print(f"> tout en ne laissant passer que {fn} arnaques sur l'ensemble du test set.'")
print("="*40)